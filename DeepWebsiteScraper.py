import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from queue import Queue, Empty
import threading
import signal
import sys
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class DeepSiteScraper:
    def __init__(self, base_url: str, search_term: str):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.search_term = search_term.lower()
        self.found = set()
        self.visited = set()
        self.lock = threading.Lock()
        self.queue = Queue()
        self.queue.put(base_url)
        self.running = True
        self.timeout = 8
        self.max_pages = 500
        self.thread_count = 20
        self.banned_extensions = {
            '.png', '.jpg', '.jpeg', '.gif', '.pdf', '.doc', '.docx',
            '.xls', '.xlsx', '.ppt', '.pptx', '.mp3', '.mp4', '.zip',
            '.tar', '.gz', '.exe', '.svg', '.css', '.js', '.ico', '.webp'
        }

        # Configure session with retries
        self.session = requests.Session()
        retries = Retry(total=3, backoff_factor=0.1,
                        status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))
        self.session.mount('http://', HTTPAdapter(max_retries=retries))
        self.session.headers = {'User-Agent': 'Mozilla/5.0'}

        signal.signal(signal.SIGINT, self._exit_gracefully)

    def _exit_gracefully(self, signum, frame):
        print("\nğŸ›‘ Deep scan is being aborted...")
        self.running = False
        sys.exit(0)

    def _is_valid_url(self, url: str) -> bool:
        parsed = urlparse(url)
        path = parsed.path.lower()
        return (
            parsed.netloc == self.domain and
            not any(path.endswith(ext) for ext in self.banned_extensions) and
            parsed.scheme in ['http', 'https']
        )

    def _fetch(self, url: str) -> str:
        try:
            with self.session.get(url, stream=True, timeout=self.timeout) as response:
                content_type = response.headers.get('Content-Type', '').lower()
                if 'text/html' not in content_type:
                    print(f"â© Skipping non-HTML: {url}")
                    return ''
                response.raise_for_status()
                return response.text
        except Exception as e:
            print(f"âš ï¸ Error at {url}: {str(e)}")
            return ''

    def _check_content(self, html: str) -> bool:
        soup = BeautifulSoup(html, 'lxml')
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'meta', 'link']):
            element.decompose()
        text = soup.get_text(separator=' ', strip=True).lower()
        return self.search_term in text

    def _extract_links(self, html: str, base_url: str) -> set:
        soup = BeautifulSoup(html, 'lxml')
        links = set()
        for link in soup.find_all('a', href=True):
            absolute_url = urljoin(base_url, link['href']).split('#')[0]
            if self._is_valid_url(absolute_url):
                links.add(absolute_url)
        return links

    def _worker(self):
        while self.running:
            try:
                current_url = self.queue.get(timeout=2)
                if current_url is None:
                    break

                # Check if already visited before processing
                with self.lock:
                    if current_url in self.visited or len(self.visited) >= self.max_pages:
                        self.queue.task_done()
                        continue
                    self.visited.add(current_url)

                print(f"ğŸŒ Scanning page {len(self.visited)}/{self.max_pages}: {current_url}")

                html = self._fetch(current_url)
                if html:
                    if self._check_content(html):
                        with self.lock:
                            self.found.add(current_url)
                            print(f"ğŸ¯ Match found on: {current_url}")

                    if len(self.visited) < self.max_pages:
                        new_links = self._extract_links(html, current_url)
                        for link in new_links:
                            with self.lock:
                                if link not in self.visited:
                                    self.queue.put(link)

                self.queue.task_done()

            except Empty:
                break
            except Exception as e:
                print(f"âš ï¸ Unexpected error: {str(e)}")
                if not self.queue.empty():
                    self.queue.task_done()

    def crawl(self):
        print(f"\nğŸ” Starting deep scan for '{self.search_term}' on {self.domain}")
        
        threads = []
        for _ in range(self.thread_count):
            t = threading.Thread(target=self._worker)
            t.start()
            threads.append(t)

        try:
            self.queue.join()
        except KeyboardInterrupt:
            self._exit_gracefully(None, None)
        finally:
            self.running = False
            for _ in range(self.thread_count):
                self.queue.put(None)
            for t in threads:
                t.join()

        print("\nğŸ“Š Final Results:")
        print(f"Search term: '{self.search_term}'")
        print(f"Scanned pages: {len(self.visited)}")
        print(f"Maximum limit: {'Reached' if len(self.visited) >= self.max_pages else 'Not reached'}")
        
        if self.found:
            print("\nâœ… Matches found on the following pages:")
            for result in self.found:
                print(f"  â†’ {result}")
        else:
            print("\nâŒ No matches found")

if __name__ == "__main__":
    print("ğŸ•¸ï¸ Deep Website Scraper")
    print("Press CTRL+C to abort\n")
    
    try:
        url = input("ğŸŒ Start URL: ").strip()
        term = input("ğŸ” Search term: ").strip()
        
        scraper = DeepSiteScraper(url, term)
        scraper.crawl()
            
    except Exception as e:
        print(f"âŒ Critical error: {str(e)}")
