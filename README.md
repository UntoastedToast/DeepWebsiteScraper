# Deep Website Scraper ğŸ•¸ï¸

A powerful, multi-threaded Python web scraper that performs deep scanning of websites to find specific search terms.

## Features âœ¨

- ğŸš€ Multi-threaded scanning for high performance
- ğŸ” Deep recursive scanning of websites
- ğŸ›¡ï¸ Built-in protection against common errors
- âš¡ Automatic retry mechanism for failed requests
- ğŸ¯ Smart filtering of non-HTML content
- ğŸ”’ Domain-restricted scanning
- â±ï¸ Configurable timeout settings
- ğŸ“Š Detailed progress reporting
- ğŸ›‘ Graceful exit with CTRL+C

## Requirements ğŸ“‹

- Python 3.6+
- Required packages:
  - requests
  - beautifulsoup4
  - lxml

## Installation ğŸ”§

1. Clone this repository:
```bash
git clone https://github.com/UntoastedToast/DeepWebsiteScraper.git
cd DeepWebsiteScraper
```

2. Install the required packages:
```bash
pip install -r requirements.txt
```

All dependencies with their correct versions will be installed automatically.

## Usage ğŸ’»

1. Run the script:
```bash
python DeepWebsiteScraper.py
```

2. Enter the required information:
   - Start URL (e.g., https://example.com)
   - Search term to find

3. The scraper will begin scanning and show real-time progress:
   - ğŸŒ Currently scanning page
   - ğŸ¯ Found matches
   - âš ï¸ Errors (if any)
   - ğŸ“Š Final results

## Configuration âš™ï¸

The following parameters can be adjusted in the `DeepSiteScraper` class:

- `timeout`: Request timeout in seconds (default: 8)
- `max_pages`: Maximum pages to scan (default: 500)
- `thread_count`: Number of concurrent threads (default: 20)
- `banned_extensions`: File types to skip

## Features in Detail ğŸ”

- **Smart Content Filtering**: Automatically skips non-HTML content and irrelevant file types
- **Domain Restriction**: Only scans pages within the specified domain
- **Error Handling**: Implements retry mechanism for failed requests
- **Resource Management**: Efficient queue-based multi-threading
- **Progress Tracking**: Real-time scanning status and results
- **Memory Efficient**: Tracks visited URLs to avoid duplicate scanning

## Error Handling ğŸ›¡ï¸

The scraper includes comprehensive error handling:
- Automatic retries for server errors (500, 502, 503, 504)
- Timeout protection
- Invalid URL detection
- Non-HTML content filtering
- Graceful exit handling

## License ğŸ“„

This project is open source and available under the MIT License.

## Contributing ğŸ¤

Contributions, issues, and feature requests are welcome!

## Notes ğŸ“

- Use responsibly and respect website robots.txt
- Configure thread count based on your system capabilities
- Adjust timeout settings based on target website response times
